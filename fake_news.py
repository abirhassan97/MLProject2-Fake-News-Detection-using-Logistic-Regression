# -*- coding: utf-8 -*-
"""Fake News.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jGYUe7o0LRWYcoNzi0lZQjsZ0uOMCtt8

# **Import**
"""

import numpy as np
import pandas as pd
# Searching words in text/paragraph
import re
# nltk -> natural language tool kit; stopwords to find & remove words with less value
from nltk.corpus import stopwords 
# Stemm our word / gives us root word
from nltk.stem.porter import PorterStemmer
# convert text to feature vector
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

"""# **What is Stopwords?**"""

import nltk
nltk.download('stopwords')

print(stopwords.words('english'))

"""# **Data Process**"""

#Load
news_data = pd.read_csv('/content/train.csv')

# checking rows , columns no.
news_data.shape
news_data.head()

#finiding missing values
news_data.isnull().sum()

"""# If lots of values are missing we can use **imputation**

for now we're replacing null with empty values
"""

# filling null values here
news_data = news_data.fillna('')

# merging author name and title
news_data['content'] = news_data['author']+' '+news_data['title']

print(news_data['content'])

# Splitting

X = news_data.drop(columns='label', axis=1)
Y = news_data['label']

print(X)
print(Y)

"""# Stemming

*Process of reducing a words to it's root word*

ie. actor, acting, action -> act 

feature vector stands for numerical data 
convertion text to numerical data is imp. to feed machine
"""

port_stem = PorterStemmer()

def stemming(content):
    # removes everything except a-z & A-Z texts from content
    stemmed_content = re.sub('[^a-zA-Z]',' ',content)
    # upper case to lower case convertion
    stemmed_content = stemmed_content.lower()
    # splitting texts & converting to list
    stemmed_content = stemmed_content.split()
    # taking each word, stemming it & removing stopwords
    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
    # joining all words
    stemmed_content = ' '.join(stemmed_content)
    return stemmed_content

news_data['content'] = news_data['content'].apply(stemming)

print(news_data['content'])

"""Splitting Content & Label"""

X = news_data['content'].values
Y = news_data['label'].values

print(X)
X.shape

print(Y)
Y.shape

"""# Converting Text to Numeric



*   tf -> term frequency
*   idf -> inverse document frequency




*counts repeating words & put values so whether those words will have meaning or not* **[too much repeatation lowers value]**
"""

vectorize = TfidfVectorizer()
vectorize.fit(X)
X = vectorize.transform(X)

print(X)

"""# **Training Model**"""

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size= 0.2, stratify = Y, random_state=2)

model = LogisticRegression()

model.fit(X_train, Y_train)

"""# **Evaluation**"""

X_train_predict = model.predict(X_train)
train_acc = accuracy_score(X_train_predict, Y_train)

print(train_acc)

X_test_predict = model.predict(X_test)
test_acc = accuracy_score(X_test_predict, Y_test)

print(test_acc)

"""# Predictive System"""

new_news = X_test[11]

prediction = model.predict(new_news)

if prediction[0]==0:
  print("এইডা ঠিক")
else:
  print("এইডা ফেইক")

print(Y_test[11])